{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class network():\n",
    "    def __init__(self,\n",
    "                input_states,\n",
    "                num_layers,\n",
    "                nurons_list,\n",
    "                output_states,\n",
    "                session):\n",
    "        \n",
    "        # Check if the number of neurons in each layer is \n",
    "        if len(nurons_list) != num_layers:\n",
    "            raise ValueError(\"nurons_list != num of layers\")\n",
    "        \n",
    "        #Initalize the session\n",
    "        self.session = session\n",
    "        \n",
    "        # Initialize the network\n",
    "        self.input_states = tf.placeholder(dtype=tf.float64, shape=[None, input_states])\n",
    "        self.target_states = tf.placeholder(dtype=tf.float64, shape=[None, output_states])\n",
    "        \n",
    "        # Create a dictonary of weight and states based on the input layers\n",
    "        # Compute the dimentions of the weight bias matrix\n",
    "        self.network_depth = num_layers\n",
    "        \n",
    "        nurons_list.append(output_states)\n",
    "        nurons_list = [input_states] + nurons_list\n",
    "        \n",
    "        self.network_width = nurons_list # list of nurons in each layer including input and output \n",
    "        \n",
    "        self.weights_bias = {}\n",
    "        for i in range(0, self.network_depth+1):\n",
    "            self.weights_bias[\"w\"+str(i)] = tf.Variable(np.random.rand(self.network_width[i], self.network_width[i+1]), dtype=tf.float64)\n",
    "            self.weights_bias[\"b\"+str(i)] = tf.Variable(np.random.rand(self.network_width[i+1]), dtype=tf.float64)\n",
    "        \n",
    "        \n",
    "        # Set the computation graph for the network\n",
    "        self.forward_pass = {}\n",
    "        # First layer\n",
    "        self.forward_pass[\"z1\"] = tf.tensordot(self.input_states, self.weights_bias[\"w0\"], axes=1) + self.weights_bias[\"b0\"]\n",
    "        self.forward_pass[\"y1\"] = tf.nn.relu(self.forward_pass[\"z1\"]) # Make this a user choice \n",
    "        \n",
    "        for i in range(2, self.network_depth+1):\n",
    "            self.forward_pass[\"z\"+str(i)] = tf.tensordot(self.forward_pass[\"y\"+str(i-1)],\n",
    "                                                         self.weights_bias[\"w\"+str(i-1)],\n",
    "                                                         axes=1) + self.weights_bias[\"b\"+str(i-1)]\n",
    "            self.forward_pass[\"y\"+str(i)] = tf.nn.relu(self.forward_pass[\"z\"+str(i)])\n",
    "            \n",
    "        # Final Layer with out activation\n",
    "        self._predict = tf.tensordot(self.forward_pass[\"y\"+str(self.network_depth)],\n",
    "                                    self.weights_bias[\"w\"+str(self.network_depth)],\n",
    "                                    axes=1) + self.weights_bias[\"b\"+str(self.network_depth)]\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss = tf.reduce_mean(tf.square(self._predict - self.target_states))\n",
    "        \n",
    "        # Optimizer \n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "        \n",
    "        # Training\n",
    "        self._train = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "        # Initialize the variables \n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def predict_on_batch(self, input_states):\n",
    "        return self.session.run(self._predict, {self.input_states:input_states})\n",
    "    \n",
    "    def fit(self, inp_states, tar_states):\n",
    "        self.session.run(self._train, {self.input_states:inp_states, self.target_states:tar_states})\n",
    "        # Return loss function.\n",
    "    \n",
    "    def save_weights(self, path):\n",
    "        data = {}\n",
    "        for i in range(0, self.network_depth+1):\n",
    "            data[\"w\"+str(i)] = self.session.run(self.weights_bias[\"w\"+str(i)])\n",
    "            data[\"b\"+str(i)] = self.session.run(self.weights_bias[\"b\"+str(i)])\n",
    "        np.save(path, data)\n",
    "        print(\"Weights Saved to \", path)\n",
    "\n",
    "    def set_weights(self, weigths_bias_load):\n",
    "        # Make sure the width and depth are equal \n",
    "        for i in range(0, self.network_depth+1):    \n",
    "            self.session.run(self.weights_bias[\"w\"+str(i)].assign(weigths_bias_load[\"w\"+str(i)]))\n",
    "            self.session.run(self.weights_bias[\"b\"+str(i)].assign(weigths_bias_load[\"b\"+str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = network(4, 1, [16], 2, tf.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
