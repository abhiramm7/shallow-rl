{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-10 15:30:37,748] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "from src import network, replay_memory_agent, deep_q_agent, epsi_greedy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deepq_session = tf.Session()\n",
    "\n",
    "q_nn = network(input_states=4,\n",
    "               num_layers=2,\n",
    "               nurons_list=[10, 10],\n",
    "               output_states=2,\n",
    "               session=deepq_session)\n",
    "\n",
    "target_nn = network(input_states=4,\n",
    "                    num_layers=2,\n",
    "                    nurons_list=[10, 10],\n",
    "                    output_states=2,\n",
    "                    session=deepq_session)\n",
    "\n",
    "target_nn.set_weights(q_nn.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay1 = replay_memory_agent(4, 5000)\n",
    "dqn_controller = deep_q_agent(action_value_model=q_nn,\n",
    "                              target_model=target_nn,\n",
    "                              states_len=4,\n",
    "                              replay_memory=replay1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Book keeping\n",
    "avg_reward_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPISODES = 10000\n",
    "# Exploration decay\n",
    "epsilon = np.linspace(0.20, 0.001, NUM_EPISODES+10)\n",
    "# Global time for target model update\n",
    "global_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0 Episode Count : 0\n",
      "Weights Saved to  model0\n",
      "8.0 Episode Count : 200\n",
      "Weights Saved to  model200\n",
      "8.0 Episode Count : 400\n",
      "Weights Saved to  model400\n",
      "9.0 Episode Count : 600\n",
      "Weights Saved to  model600\n",
      "10.0 Episode Count : 800\n",
      "Weights Saved to  model800\n",
      "22.0 Episode Count : 1000\n",
      "Weights Saved to  model1000\n",
      "14.0 Episode Count : 1200\n",
      "Weights Saved to  model1200\n",
      "12.0 Episode Count : 1400\n",
      "Weights Saved to  model1400\n",
      "8.0 Episode Count : 1600\n",
      "Weights Saved to  model1600\n",
      "12.0 Episode Count : 1800\n",
      "Weights Saved to  model1800\n",
      "10.0 Episode Count : 2000\n",
      "Weights Saved to  model2000\n",
      "12.0 Episode Count : 2200\n",
      "Weights Saved to  model2200\n",
      "9.0 Episode Count : 2400\n",
      "Weights Saved to  model2400\n",
      "13.0 Episode Count : 2600\n",
      "Weights Saved to  model2600\n",
      "9.0 Episode Count : 2800\n",
      "Weights Saved to  model2800\n",
      "10.0 Episode Count : 3000\n",
      "Weights Saved to  model3000\n",
      "9.0 Episode Count : 3200\n",
      "Weights Saved to  model3200\n",
      "9.0 Episode Count : 3400\n",
      "Weights Saved to  model3400\n",
      "9.0 Episode Count : 3600\n",
      "Weights Saved to  model3600\n",
      "8.0 Episode Count : 3800\n",
      "Weights Saved to  model3800\n",
      "9.0 Episode Count : 4000\n",
      "Weights Saved to  model4000\n",
      "8.0 Episode Count : 4200\n",
      "Weights Saved to  model4200\n",
      "9.0 Episode Count : 4400\n",
      "Weights Saved to  model4400\n",
      "10.0 Episode Count : 4600\n",
      "Weights Saved to  model4600\n",
      "11.0 Episode Count : 4800\n",
      "Weights Saved to  model4800\n",
      "15.0 Episode Count : 5000\n",
      "Weights Saved to  model5000\n",
      "14.0 Episode Count : 5200\n",
      "Weights Saved to  model5200\n",
      "10.0 Episode Count : 5400\n",
      "Weights Saved to  model5400\n",
      "36.0 Episode Count : 5600\n",
      "Weights Saved to  model5600\n",
      "31.0 Episode Count : 5800\n",
      "Weights Saved to  model5800\n",
      "22.0 Episode Count : 6000\n",
      "Weights Saved to  model6000\n",
      "16.0 Episode Count : 6200\n",
      "Weights Saved to  model6200\n",
      "9.0 Episode Count : 6400\n",
      "Weights Saved to  model6400\n",
      "10.0 Episode Count : 6600\n",
      "Weights Saved to  model6600\n",
      "8.0 Episode Count : 6800\n",
      "Weights Saved to  model6800\n",
      "23.0 Episode Count : 7000\n",
      "Weights Saved to  model7000\n",
      "16.0 Episode Count : 7200\n",
      "Weights Saved to  model7200\n",
      "10.0 Episode Count : 7400\n",
      "Weights Saved to  model7400\n",
      "44.0 Episode Count : 7600\n",
      "Weights Saved to  model7600\n",
      "11.0 Episode Count : 7800\n",
      "Weights Saved to  model7800\n",
      "72.0 Episode Count : 8000\n",
      "Weights Saved to  model8000\n",
      "9.0 Episode Count : 8200\n",
      "Weights Saved to  model8200\n",
      "9.0 Episode Count : 8400\n",
      "Weights Saved to  model8400\n",
      "10.0 Episode Count : 8600\n",
      "Weights Saved to  model8600\n",
      "10.0 Episode Count : 8800\n",
      "Weights Saved to  model8800\n"
     ]
    }
   ],
   "source": [
    "for episodes in range(0, NUM_EPISODES):\n",
    "    \n",
    "    # Initial State\n",
    "    state = env.reset()\n",
    "    done=False\n",
    "    # Clear the reward buffer\n",
    "    rewards = []\n",
    "    \n",
    "    while not(done):\n",
    "        global_time += 1\n",
    "\n",
    "        # Reshape the state\n",
    "        state = np.asarray(state)\n",
    "        state = state.reshape(1,4)\n",
    "\n",
    "        # Pick a action based on the state\n",
    "        q_values = q_nn.predict_on_batch(state)\n",
    "        action = epsi_greedy([0, 1], q_values, epsilon[episodes])\n",
    "\n",
    "        # Implement action and observe the reward signal\n",
    "        state_new, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state_new = np.asarray(state_new)\n",
    "        state_new = state_new.reshape(1,4)\n",
    "\n",
    "        # Update the replay memory\n",
    "        replay1.replay_memory_update(state, state_new, reward, action, done)\n",
    "\n",
    "        # Train\n",
    "        update = True if global_time%5000==0 else False\n",
    "        dqn_controller.train_q(update)\n",
    "\n",
    "        state = state_new\n",
    "\n",
    "    avg_reward_episodes.append(sum(rewards))\n",
    "    \n",
    "    if episodes%200 == 0:\n",
    "        print(sum(rewards), \"Episode Count :\" ,episodes)\n",
    "        q_nn.save_weights(\"model\"+str(episodes))\n",
    "        np.save(\"sum_rewards\"+str(episodes), avg_reward_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
