{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-11 20:51:01,179] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0 Episode Count : 0\n",
      "11.0 Episode Count : 100\n",
      "13.0 Episode Count : 200\n",
      "11.0 Episode Count : 300\n",
      "10.0 Episode Count : 400\n",
      "11.0 Episode Count : 500\n",
      "9.0 Episode Count : 600\n",
      "11.0 Episode Count : 700\n",
      "15.0 Episode Count : 800\n",
      "Updated : 10000\n",
      "14.0 Episode Count : 900\n",
      "9.0 Episode Count : 1000\n",
      "10.0 Episode Count : 1100\n",
      "9.0 Episode Count : 1200\n",
      "Updated : 15000\n",
      "10.0 Episode Count : 1300\n",
      "9.0 Episode Count : 1400\n",
      "11.0 Episode Count : 1500\n",
      "10.0 Episode Count : 1600\n"
     ]
    }
   ],
   "source": [
    "from src import replay_memory_agent, deep_q_agent, epsi_greedy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "def build_network(input_states,\n",
    "                  output_states,\n",
    "                  hidden_layers,\n",
    "                  nuron_count,\n",
    "                  activation_function,\n",
    "                  dropout):\n",
    "    \"\"\"\n",
    "    Build and initialize the neural network with a choice for dropout\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nuron_count, input_dim=input_states))\n",
    "    model.add(Activation(activation_function))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i_layers in range(0, hidden_layers - 1):\n",
    "        model.add(Dense(nuron_count))     \n",
    "        model.add(Activation(activation_function))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(output_states))\n",
    "    sgd = Adam(lr=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "    return model\n",
    "\n",
    "q_nn = build_network(4, 2, 5, 20, \"relu\", 0.0);\n",
    "#q_nn.load_weights(\"model_1\")\n",
    "target_nn = build_network(4, 2, 5, 20, \"relu\", 0.0);\n",
    "target_nn.set_weights(q_nn.get_weights())\n",
    "\n",
    "replay1 = replay_memory_agent(4, 5000)\n",
    "\n",
    "his = LossHistory()\n",
    "\n",
    "dqn_controller = deep_q_agent(action_value_model=q_nn,\n",
    "                              target_model=target_nn,\n",
    "                              states_len=4,\n",
    "                              replay_memory=replay1,\n",
    "                              call = [his])\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Book keeping\n",
    "avg_reward_episodes = []\n",
    "# Global time step\n",
    "gt = 0\n",
    "\n",
    "for episodes in range(0, 5000):\n",
    "\n",
    "    # Initial State\n",
    "    state = env.reset()\n",
    "    done=False\n",
    "    \n",
    "    # Clear the reward buffer\n",
    "    rewards = []\n",
    "    if gt > 10000:\n",
    "        epsilon = max(0.01, epsilon-0.0009)\n",
    "    else:\n",
    "        epsilon = 0.20\n",
    "    \n",
    "    episode_time = 0\n",
    "\n",
    "    while not(done):\n",
    "        gt += 1\n",
    "\n",
    "        # Reshape the state\n",
    "        state = np.asarray(state)\n",
    "        state = state.reshape(1,4)\n",
    "\n",
    "        # Pick a action based on the state\n",
    "        q_values = q_nn.predict_on_batch(state)\n",
    "\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.choice([0, 1])\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "\n",
    "        # Implement action and observe the reward signal\n",
    "        state_new, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # Update the replay memory\n",
    "        replay1.replay_memory_update(state, state_new, reward, action, done)\n",
    "\n",
    "        if gt > 5000:\n",
    "            update = True if gt%5000==0 else False\n",
    "            dqn_controller.train_q(update)\n",
    "            if update:\n",
    "                print(\"Updated :\",gt)\n",
    "\n",
    "        state = state_new\n",
    "\n",
    "        episode_time += 1\n",
    "        if episode_time >= 200:\n",
    "            break\n",
    "\n",
    "    avg_reward_episodes.append(sum(rewards))\n",
    "    if episodes%100 == 0:\n",
    "        print(sum(rewards), \"Episode Count :\" ,episodes)\n",
    "        q_nn.save_weights(\"model\"+str(episodes))\n",
    "\n",
    "np.save(\"sum_rewards\", avg_reward_episodes)\n",
    "plt.plot(avg_reward_episodes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LossHistory at 0x11e869f28>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.6758557e+10,\n",
       " 3.6758553e+10,\n",
       " 3.6758479e+10,\n",
       " 3.6758282e+10,\n",
       " 3.6758086e+10,\n",
       " 3.6757799e+10,\n",
       " 3.6757426e+10,\n",
       " 3.6757066e+10,\n",
       " 3.6756586e+10,\n",
       " 3.6756152e+10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "his.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1649.91485255,  1584.63693889]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2358904.1307523968"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
